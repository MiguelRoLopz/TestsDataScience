{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de clasificación\n",
    "\n",
    "En esta segunda parte del Test 1 de clasificación, nos centraremos en elegir los modelos que se adapten mejor al tipo de datos que hemos preparado en el primer script EDA, analizar sus resultados y justificar la elección en función de métricas clave de rendimiento.\n",
    "\n",
    "Para obtener unos resultados óptimos en este problema de clasificación sin incurrir en tiempos de procesamiento desmedidos, probaremos a implementar los siguientes modelos:\n",
    "\n",
    "· Regresión logística: Intentaremos obtener una referencia inicial de accuracy, precision, ROC-AUC y recall. Es un modelo rápido de entrenar y sencillo de interpretar, por lo que nos dará una idea inicial del comportamiento de los datos.\n",
    "\n",
    "· Ramdom Forest: Entrenaremos un conjunto de árboles de decisión con muestras aleatorias del Dataset para intentar obtener una clasificación más profunda de los datos.\n",
    "\n",
    "· Gradient Boosting (XGBoost): Intentaremos maximizar la precisión (accuracy) utilizando este modelo que combina el entrenamiento secuencial de árboles de decisión con modelos más fuertes.\n",
    "\n",
    "## Preparación de los datos\n",
    "\n",
    "· One-hot encoding para las variables categóricas\n",
    "\n",
    "· Separación en datos de entrenamiento y test\n",
    "\n",
    "· Estandarización de las variables numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el Dataset limpio\n",
    "df = pd.read_csv('dataset_EDA.csv')\n",
    "\n",
    "# Definimos características de la variable objetivo\n",
    "X = df.drop(['symboling'], axis=1)\n",
    "y = df['symboling']\n",
    "\n",
    "# Definimos las columnas categóricas y numéricas para el preprocesamiento\n",
    "categorical_columns = ['make', 'fuel-type', 'aspiration', 'num-of-doors', 'body-style', \n",
    "                       'drive-wheels', 'engine-location', 'horsepower_binned', 'price_binned']\n",
    "\n",
    "numeric_columns = ['normalized-losses', 'wheel-base', 'length', 'width', 'height', \n",
    "                   'curb-weight', 'engine-size', 'bore', 'stroke', 'compression-ratio', \n",
    "                   'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'power_to_weight', 'make_price_ratio']\n",
    "\n",
    "# Dividimos el conjunto de datos en entrenamiento (80%) y test (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creamos un transformador para aplicar One-hot encoding a las variables categóricas y estandarización a las numéricas\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_columns),\n",
    "        ('cat', OneHotEncoder(), categorical_columns)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el pipeline de preprocesamiento\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y evaluación de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regresión Logística:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -2       1.00      1.00      1.00         1\n",
      "          -1       0.83      1.00      0.91         5\n",
      "           0       0.87      0.72      0.79        18\n",
      "           1       0.56      0.62      0.59         8\n",
      "           2       0.50      0.60      0.55         5\n",
      "           3       0.75      0.75      0.75         4\n",
      "\n",
      "    accuracy                           0.73        41\n",
      "   macro avg       0.75      0.78      0.76        41\n",
      "weighted avg       0.75      0.73      0.74        41\n",
      "\n",
      "ROC-AUC: 0.9216106422628162\n"
     ]
    }
   ],
   "source": [
    "# Entrenamos el modelo de regresión logística\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_logreg = logreg.predict(X_test_preprocessed)\n",
    "\n",
    "# Métricas de evaluación\n",
    "print(\"Regresión Logística:\")\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, logreg.predict_proba(X_test_preprocessed), multi_class='ovr'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -2       1.00      1.00      1.00         1\n",
      "          -1       0.83      1.00      0.91         5\n",
      "           0       0.93      0.72      0.81        18\n",
      "           1       0.54      0.88      0.67         8\n",
      "           2       1.00      0.40      0.57         5\n",
      "           3       0.80      1.00      0.89         4\n",
      "\n",
      "    accuracy                           0.78        41\n",
      "   macro avg       0.85      0.83      0.81        41\n",
      "weighted avg       0.84      0.78      0.78        41\n",
      "\n",
      "ROC-AUC: 0.9774502269067487\n"
     ]
    }
   ],
   "source": [
    "# Entrenamos el modelo Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_rf = rf.predict(X_test_preprocessed)\n",
    "\n",
    "# Métricas de evaluación\n",
    "print(\"Random Forest:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, rf.predict_proba(X_test_preprocessed), multi_class='ovr'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [-2 -1  0  1  2  3]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Entrenamos el modelo XGBoost\u001b[39;00m\n\u001b[0;32m      2\u001b[0m xgboost_model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(use_label_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlogloss\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mxgboost_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_preprocessed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Predicciones\u001b[39;00m\n\u001b[0;32m      6\u001b[0m y_pred_xgb \u001b[38;5;241m=\u001b[39m xgboost_model\u001b[38;5;241m.\u001b[39mpredict(X_test_preprocessed)\n",
      "File \u001b[1;32mc:\\Users\\rtx11679\\Documents\\HybridIntelligence\\MRL-TestDataScience-1\\env1\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rtx11679\\Documents\\HybridIntelligence\\MRL-TestDataScience-1\\env1\\Lib\\site-packages\\xgboost\\sklearn.py:1491\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1486\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1488\u001b[0m     classes\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1489\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (classes \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m   1490\u001b[0m ):\n\u001b[1;32m-> 1491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1492\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1493\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1494\u001b[0m     )\n\u001b[0;32m   1496\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [-2 -1  0  1  2  3]"
     ]
    }
   ],
   "source": [
    "# Entrenamos el modelo XGBoost\n",
    "xgboost_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "xgboost_model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_xgb = xgboost_model.predict(X_test_preprocessed)\n",
    "\n",
    "# Métricas de evaluación\n",
    "print(\"XGBoost:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, xgboost_model.predict_proba(X_test_preprocessed), multi_class='ovr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"SUCIOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO!!!!!!!!!!!!!!\n",
    "\n",
    "# 1. One-hot encoding para variables categóricas\n",
    "categoric_columns = df.select_dtypes(include=['object']).columns\n",
    "df = pd.get_dummies(df, columns=categoric_columns)\n",
    "\n",
    "# Definir las características (X) y la variable objetivo (y)\n",
    "X = df.drop('symboling', axis=1)  # Consideramos \"symboling\" como la variable objetivo\n",
    "y = df['symboling']\n",
    "\n",
    "# Dividir el dataset en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 2. Estandarización de los datos\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 3. Entrenamiento y evaluación de los modelos de clasificación\n",
    "\n",
    "# Modelo 1: Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo Random Forest\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Random Forest Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "print(\"Random Forest Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "# Modelo 2: Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo Logistic Regression\n",
    "print(\"\\nLogistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"Logistic Regression Report:\\n\", classification_report(y_test, y_pred_lr))\n",
    "print(\"Logistic Regression Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lr))\n",
    "\n",
    "# Modelo 3: Support Vector Machine (SVM)\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo SVM\n",
    "print(\"\\nSVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"SVM Report:\\n\", classification_report(y_test, y_pred_svm))\n",
    "print(\"SVM Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_svm))\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
